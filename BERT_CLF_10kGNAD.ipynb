{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT CLF 10kGNAD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM8DVqEVXQMuuPy36craTEi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "len8Qi91y5FX",
        "colab_type": "code",
        "outputId": "b85601f3-3b8d-45e4-9d15-7cd3ce7c01de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!pip install wget\n",
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QA9w6dbwfzz",
        "colab_type": "text"
      },
      "source": [
        "# BERT Fine Tuning for Multi Class Text Classification \n",
        "\n",
        "This notebook contains code to fine tune a pretrained BERT language model to a specific classification task. \n",
        "As BERT model interface the Huggingface library with a PyTorch backend is used.\n",
        "\n",
        "In this notebook, the model has been fine tuned with the German 10kGNAD Dataset but could easily be fine tuned on any other classification dataset.\n",
        "\n",
        "The code was implemented based on the huggingface example scripts for glue tasks fine tuning (https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128) and the blog post by Chris McCormick (http://mccormickml.com/2019/07/22/BERT-fine-tuning/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKjjVxVDKsS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors\n",
        "import seaborn as sns\n",
        "import IPython\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO5bX3f4kOcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Empty cache of GPU\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhzCltg3tR-q",
        "colab_type": "code",
        "outputId": "401f14ff-0283-441d-a01c-82c6635367ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZxf_D2gAdTO",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJzGKGLX9AxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download dataset\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/tblock/10kGNAD/master/articles.csv\",\n",
        "                 encoding=\"utf-8\",\n",
        "                 delimiter=\";\",\n",
        "                 quotechar=\"'\", names=[\"label\",\"text\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuN3olL1s1R0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = df.text.values\n",
        "label_cats = df.label.astype('category').cat\n",
        "\n",
        "# List of label names (str)\n",
        "label_names = label_cats.categories\n",
        "\n",
        "# List of label ids (int, in range (0,num_classes-1))\n",
        "labels = label_cats.codes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSsp_l_yoNJf",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UUTkMV4yDTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_INPUT_LENGTH = 192"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-OE6oDZoMIG",
        "colab_type": "code",
        "outputId": "37ad757d-4941-4d9e-8103-608f5c015fa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load the pretrained BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased', do_lower_case=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDkMb1urrIAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to their word IDs\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,            \n",
        "                        add_special_tokens = True,\n",
        "                        max_length = MAX_INPUT_LENGTH,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt')\n",
        "  \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBM6SwDHvirL",
        "colab_type": "code",
        "outputId": "57966202-e6dd-4445-e6b6-7dce140d6190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', texts[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "print('Attention Mask:', attention_masks[0]) # 1 for all text tokens, 0 for all padding tokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-0d953e2e0b24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mattention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Print sentence 0, now as a list of IDs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cat() received an invalid combination of arguments - got (Tensor, dim=int), but expected one of:\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO3MaaVkuI-c",
        "colab_type": "code",
        "outputId": "513c6fa2-2169-4e8f-aeae-39d852ee64c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 80-10-10 train-validation-test split\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset)) \n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "print(f\"{train_size} training samples\")\n",
        "print(f\"{val_size} validation samples\")\n",
        "print(f\"{test_size} test samples\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8218 training samples\n",
            "1027 validation samples\n",
            "1028 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiHoD0KoNpZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "# For test the order doesn't matter, so we'll just read them sequentially.\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            sampler = SequentialSampler(test_dataset),\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1IY9cwiAUCZ",
        "colab_type": "text"
      },
      "source": [
        "## Create and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWfvlMlBg-3_",
        "colab_type": "code",
        "outputId": "6b5d087f-29d7-4f61-f31f-5edea92940c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-german-cased\",\n",
        "    num_labels = len(label_names),\n",
        "    output_attentions = True, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIBG_g36utJA",
        "colab_type": "code",
        "outputId": "52e33119-d970-48a3-eb89-e5e9fc1e129e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30000, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (9, 768)\n",
            "classifier.bias                                                 (9,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzojbxacu1ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFeBj0EhyGlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs5ImW58hZ4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbwgopNUhm-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub1-0NFChoc6",
        "colab_type": "code",
        "outputId": "0232822f-ee68-4cf4-8156-617abbc90977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits, attentions = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits, attentions) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of    514.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    514.    Elapsed: 0:00:25.\n",
            "  Batch   120  of    514.    Elapsed: 0:00:37.\n",
            "  Batch   160  of    514.    Elapsed: 0:00:50.\n",
            "  Batch   200  of    514.    Elapsed: 0:01:02.\n",
            "  Batch   240  of    514.    Elapsed: 0:01:15.\n",
            "  Batch   280  of    514.    Elapsed: 0:01:27.\n",
            "  Batch   320  of    514.    Elapsed: 0:01:39.\n",
            "  Batch   360  of    514.    Elapsed: 0:01:52.\n",
            "  Batch   400  of    514.    Elapsed: 0:02:04.\n",
            "  Batch   440  of    514.    Elapsed: 0:02:16.\n",
            "  Batch   480  of    514.    Elapsed: 0:02:29.\n",
            "\n",
            "  Average training loss: 0.60\n",
            "  Training epcoh took: 0:02:39\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.88\n",
            "  Validation Loss: 0.36\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    514.    Elapsed: 0:00:12.\n",
            "  Batch    80  of    514.    Elapsed: 0:00:25.\n",
            "  Batch   120  of    514.    Elapsed: 0:00:37.\n",
            "  Batch   160  of    514.    Elapsed: 0:00:49.\n",
            "  Batch   200  of    514.    Elapsed: 0:01:02.\n",
            "  Batch   240  of    514.    Elapsed: 0:01:14.\n",
            "  Batch   280  of    514.    Elapsed: 0:01:27.\n",
            "  Batch   320  of    514.    Elapsed: 0:01:39.\n",
            "  Batch   360  of    514.    Elapsed: 0:01:51.\n",
            "  Batch   400  of    514.    Elapsed: 0:02:04.\n",
            "  Batch   440  of    514.    Elapsed: 0:02:16.\n",
            "  Batch   480  of    514.    Elapsed: 0:02:28.\n",
            "\n",
            "  Average training loss: 0.23\n",
            "  Training epcoh took: 0:02:39\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.89\n",
            "  Validation Loss: 0.34\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:05:30 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxik8IiPhvXC",
        "colab_type": "code",
        "outputId": "9f4780d8-35b8-4d40-97d4-5bf2f0088703",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.60</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0:02:39</td>\n",
              "      <td>0:00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0:02:39</td>\n",
              "      <td>0:00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.60         0.36           0.88       0:02:39         0:00:06\n",
              "2               0.23         0.34           0.89       0:02:39         0:00:06"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2G-EegN85iC",
        "colab_type": "code",
        "outputId": "e0723a78-97a6-4851-c0e7-2ee84718db8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWmaN_QFZKvd",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhhTNEe2ZQM-",
        "colab_type": "code",
        "outputId": "ae2e34fb-cd65-4ca4-ec40-ba283e1bf665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print(\"Predicting labels ...\")\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in test_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.extend(logits)\n",
        "  true_labels.extend(label_ids)\n",
        "\n",
        "print(f\"Predicted {len(predictions)} samples\")"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels ...\n",
            "Predicted 1028 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htdXUiYk7fyB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ba442cda-4380-40cb-acc6-4fdb733c7b13"
      },
      "source": [
        "predictions = np.argmax(predictions, axis=1)\n",
        "print(f\"Test set accuracy: {accuracy_score(true_labels, predictions)}\")\n",
        "print(f\"Test set Matthews correlation coefficient: {matthews_corrcoef(true_labels, predictions)}\")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set accuracy: 0.9377431906614786\n",
            "Test set Matthews correlation coefficient: 0.9284684382105135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5EStuaZBGLk",
        "colab_type": "text"
      },
      "source": [
        "## Save model to disk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZIqwQvuBKFZ",
        "colab_type": "code",
        "outputId": "af3f24fe-b001-4f0f-c1ad-dd706abde7eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "#torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/vocab.txt',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le22WIOxhSaj",
        "colab_type": "code",
        "outputId": "b671c38b-51d6-402a-ba2a-1f768aacb8fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Mount Google Drive to this Notebook instance.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaXz2CrQhVyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy the model files to a directory in your Google Drive.\n",
        "!cp -r ./model_save/ \"./drive/My Drive/NLP/BERT CLF 10kGNAD/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tXFWVdxh7Zd",
        "colab_type": "text"
      },
      "source": [
        "## Load Model from Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94-4pMG-h_G_",
        "colab_type": "code",
        "outputId": "bee2a0c7-b96c-478f-b11d-9ac193542032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "output_dir = './drive/My Drive/NLP/BERT CLF 10kGNAD/'\n",
        "\n",
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT525N9VbR8n",
        "colab_type": "text"
      },
      "source": [
        "## Predict on some sample text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI_uBpaWxZaY",
        "colab_type": "text"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esq849IYbVj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = [\"Heldt hätte kein Problem damit, wenn es in zwei Wochen in der Bundesliga wieder losgehen würde. „Wir lernen gerade alle, Kompromisse einzugehen, an die wir vor Wochen noch nicht gedacht haben.“ Natürlich seien zehn bis 14 Tage richtiges Mannschaftstraining sinnvoll. „Aber vielleicht kriegen wir diese Zeit nicht. Und dann machen wir es so“, ergänzte der langjährige Manager. Köln-Profi Birger Verstraete hält die Maßnahmen nach den drei positiven Corona-Tests derweil für leichtsinnig. „Wir sollten vorerst nicht unter Quarantäne gestellt werden, und das ist ein bisschen bizarr“, sagte der belgische Mittelfeldspieler dem TV-Sender „VTM“. Beim 1. FC Köln waren zwei Spieler und ein Betreuer positiv auf das Coronavirus getestet worden. „Der Physiotherapeut ist der Mann, der mich und andere Spieler wochenlang behandelt hat. Und mit einem der beiden fraglichen Spieler habe ich am Donnerstag im Fitnessstudio ein Duo gebildet“, sagte Verstraete in dem Interview, über das „Het Laatste Nieuws berichtete.\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2PSbpD9bcqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in texts:\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "                          text,            \n",
        "                          add_special_tokens = True,\n",
        "                          max_length = 64,\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,\n",
        "                          return_tensors = 'pt')\n",
        "  \n",
        "  # Add the encoded sentence to the list.    \n",
        "  input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "  # And its attention mask (simply differentiates padding from non-padding).\n",
        "  attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBv57F46cKST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "last_layer_attentions = []\n",
        "\n",
        "# Move input ids and attention masks to GPU\n",
        "input_ids = input_ids.to(device)\n",
        "attention_masks = attention_masks.to(device)\n",
        "\n",
        "for i in range(len(input_ids)):\n",
        "\n",
        "  ids = input_ids[i].unsqueeze(0)\n",
        "  masks = attention_masks[i].unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(ids, token_type_ids=None, \n",
        "                      attention_mask=masks)\n",
        "\n",
        "  # Get logits and compute softmax\n",
        "  logits = outputs[0]\n",
        "  logits = torch.softmax(logits,dim=1)\n",
        "  last_layer_attention = outputs[1][-1]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  last_layer_attention = last_layer_attention.detach().cpu().numpy()\n",
        "\n",
        "  last_layer_attentions.append(last_layer_attention)\n",
        "  predictions.append(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8t_Ygply0X3",
        "colab_type": "code",
        "outputId": "c80c0c4c-a5d8-427d-dbb0-f70e610c3dec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "probs = predictions[0][0]\n",
        "print(\"text:\", texts[0])\n",
        "print(\"predictions:\", probs)\n",
        "pred_idx = np.argmax(probs)\n",
        "print(f\"Prediction: {label_names[pred_idx]} ({probs[pred_idx]:.2f})\", )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text: Heldt hätte kein Problem damit, wenn es in zwei Wochen in der Bundesliga wieder losgehen würde. „Wir lernen gerade alle, Kompromisse einzugehen, an die wir vor Wochen noch nicht gedacht haben.“ Natürlich seien zehn bis 14 Tage richtiges Mannschaftstraining sinnvoll. „Aber vielleicht kriegen wir diese Zeit nicht. Und dann machen wir es so“, ergänzte der langjährige Manager. Köln-Profi Birger Verstraete hält die Maßnahmen nach den drei positiven Corona-Tests derweil für leichtsinnig. „Wir sollten vorerst nicht unter Quarantäne gestellt werden, und das ist ein bisschen bizarr“, sagte der belgische Mittelfeldspieler dem TV-Sender „VTM“. Beim 1. FC Köln waren zwei Spieler und ein Betreuer positiv auf das Coronavirus getestet worden. „Der Physiotherapeut ist der Mann, der mich und andere Spieler wochenlang behandelt hat. Und mit einem der beiden fraglichen Spieler habe ich am Donnerstag im Fitnessstudio ein Duo gebildet“, sagte Verstraete in dem Interview, über das „Het Laatste Nieuws berichtete.\n",
            "predictions: [0.00266012 0.00237545 0.00650952 0.00293079 0.00180753 0.976218\n",
            " 0.001346   0.00354758 0.00260511]\n",
            "Prediction: Sport (0.98)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi9mhLaryMNS",
        "colab_type": "code",
        "outputId": "c80eaddd-fdf4-4162-ef02-089a207eb5e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "fig = plt.figure(tight_layout=True)\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "y_pos = np.arange(len(label_names))\n",
        "confidences = [probs[i] for i in range(len(label_names))]\n",
        "\n",
        "ax.barh(y_pos, confidences, align='center')\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(label_names)\n",
        "ax.invert_yaxis()  # labels read top-to-bottom\n",
        "ax.set_xlabel('Confidence')\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0cAAAGXCAYAAAB4GyuFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeVRV5f7H8Q/zICBJOCFophwMBA6DczmkOVBhllcxsRRTuyWmtwFvrlYOXdOb6U8oNVObzEylzBwry4nS7OBQaGo5T+EIGCDC+f3h4lxPoB4RhOT9Wuuu5Xn2s5/93fYs1/2sZ+9n25nNZrMAAAAAoJqzr+wCAAAAAKAqIBwBAAAAgAhHAAAAACCJcAQAAAAAkghHAAAAACCJcAQAAAAAkghHAAAAACBJcqzsAlA5zp69oKIiPnGFq/Px8dDp0zmVXQaqOOYJbME8gS2YJ7DFzc4Te3s73XFHjaseJxxVU0VFZsIRros5AlswT2AL5glswTyBLSpynvBYHQAAAACIcAQAAAAAkghHAAAAACCJcAQAAAAAkghHAAAAACCJcAQAAAAAkghHAAAAACCJcAQAAAAAkghHAAAAACCJcAQAAAAAkghHAAAAACCJcAQAAAAAkghH1ZaPj4c8vdwquwwAAACgyiAcVVMJE9bI1cWxsssAAAAAqgzCEQAAAACIcAQAAAAAkghHAAAAACCJcAQAAAAAkghHAAAAACCJcAQAAAAAkghHAAAAACCJcFTuUlNTZTAYrvq/PXv2KD8/X8nJydq8eXOZr7Nt2zYlJycrKyurHKsHAAAAqi++AlpBRo4cqXr16pVor1evnvLz85WSkqJnn31WLVu2LNP427ZtU0pKih555BF5eXndbLkAAABAtUc4qiDt27dXs2bNSj3Gag8AAABQ9fBY3S125MgRRUdHS5JSUlIsj9slJydLknbv3q2kpCTdf//9at68udq2bavRo0fr7NmzljGSk5M1ceJESdL9999vGePIkSO3/oYAAACA2wQrRxUkKytLZ86csWqzt7dXrVq1NG7cOL3yyivq0qWLunTpIkkyGAySpLS0NB0+fFi9evWSr6+v9u7dq08//VT79u3Tp59+Kjs7O3Xp0kWHDh3SF198odGjR+uOO+6QJNWqVevW3iQAAABwGyEcVZABAwaUaPP29tbmzZvVvXt3vfLKKzIYDIqNjbXq069fPw0aNMiqLTw8XKNGjdJPP/2kqKgoBQUFKTg4WF988YU6d+6sBg0alLlOX1/PMp+L2x/zA7ZgnsAWzBPYgnkCW1TkPCEcVZCxY8cqICDAqs3Jyem657m6ulr+nJ+frwsXLigsLEyS9MsvvygqKqpc68zMzC7X8XD78PX1ZH7gupgnsAXzBLZgnsAWNztP7O3t5OPjcdXjhKMKEhYWdtUNGa7l3LlzSklJ0YoVK3T69GmrY9nZ/IMBAAAAVBTCURXz3HPPKT09XQkJCWrWrJnc3d1VVFSkwYMHy2w2V3Z5AAAAwG2LcFQJ7OzsSm0/f/68vv/+ew0fPlzPPvuspf3AgQM2jwEAAACgbNjKuxK4uLjIzs6uxPeOHBwcSu3//vvvl2hzd3eXxKN2AAAAQHlh5aiCrFu3Tnv27CnR3rJlS9WtW1eBgYFauXKlGjVqJG9vbzVt2lSBgYGKjo7Wu+++q4KCAtWpU0ebNm0q9ftFwcHBkqSpU6eqR48ecnJyUseOHS2hCQAAAMCNIRxVkKlTp5ba/tZbb6lu3boaP368xo0bp9dff10XL17Us88+q8DAQE2ZMkXjx4/Xxx9/LLPZrLZt22r27Nm69957rca55557NGrUKM2fP18bNmxQUVGRvvnmG8IRAAAAUEZ2Zt7yr5YSJqzRnDEPsGUmrootVWEL5glswTyBLZgnsEVFb+XNO0cAAAAAIMIRAAAAAEgiHAEAAACAJMIRAAAAAEgiHAEAAACAJMIRAAAAAEgiHFVbc8Y8oLz8S5VdBgAAAFBl8BHYaur06RwVFfGJKwAAAKAYK0cAAAAAIMIRAAAAAEgiHAEAAACAJMIRAAAAAEgiHAEAAACAJMJRteXj4yFPL7fKLgMAAACoMghH1VTChDVydWEndwAAAKAY4QgAAAAARDgCAAAAAEmEIwAAAACQRDgCAAAAAEmEIwAAAACQRDgCAAAAAEmEo3KxefNmGQwGbd68+ZZf+8iRIzIYDEpNTb3l1wYAAABuJ4SjK6SmpspgMGjXrl2VXQoAAACAW4xwBAAAAAAiHAEAAACAJMLRNcXHxys2NlZ79uxRfHy8wsLCdO+992r27NnXPXfr1q1KTExUhw4dFBISovbt2+s///mP8vLyrPolJSUpKipKx48f17Bhw2Q0GtWqVStNmjRJhYWFVn2zsrKUlJSkyMhIRUVF6aWXXlJ2dna53jMAAABQXTlWdgFV3blz5zR48GB169ZN3bt316pVq/TGG28oMDBQ7du3v+p5q1atUl5enuLi4uTt7a0dO3boo48+0okTJzR9+nSrvpcuXdKgQYMUERGhF198UWlpaZo7d678/f3Vr18/SZLZbNY///lP/fTTT4qLi1Pjxo311Vdf6aWXXqrQ+wcAAACqC8LRdZw4cUJTpkzRgw8+KEl67LHH1KlTJy1ZsuSa4ej555+Xq6ur5XefPn3UsGFDvfnmmzp27Jjq169vOZabm6uePXtq6NChkqS4uDg98sgjWrx4sSUcffPNN/rxxx81evRoPfnkk5Z+AwYMuKn78/X1vKnzcXtjfsAWzBPYgnkCWzBPYIuKnCeEo+vw9PRUTEyM5bezs7OaN2+uw4cPX/O8K4PRn3/+qby8PBmNRpnNZmVkZFiFI+lyeLpSZGSkvvjiC8vv9evXy8nJyaqfg4OD+vfvr61bt5bp3iQpM5PH8lA6X19P5geui3kCWzBPYAvmCWxxs/PE3t5OPj4eVz1OOLqOevXqyc7OzqqtZs2a+vXXX6953rFjxzR9+nStXbtW58+ftzqWk5Nj9dvd3V3e3t4lrnHleUePHlWdOnXk5uZm1e+uu+6y+V4AAAAAXB3h6Drs7W98z4rCwkINHDhQ58+f1+DBg9W4cWO5u7vr5MmTSkpKUlFRkVV/BweH8ioXAAAAQBkRjirAnj17dODAAU2aNEk9e/a0tG/atKnMY/r5+Wnz5s3Kzc21Wj3av3//TdUKAAAA4DK28q4AxatNZrPZ0mY2m/XBBx+Uecz77rtPBQUFWrhwoaWtsLBQH330UdkLBQAAAGDBylEFaNy4sQICAjRp0iSdPHlSHh4eWr16tbKysso8ZqdOnRQREaFJkybp0KFDuvvuu7VmzRq+cwQAAACUE1aOKoCTk5NmzpypZs2aadasWUpJSVGjRo00adKkMo9pb2+vGTNm6KGHHtLSpUs1depU1alT56bGBAAAAPA/duYrn/1CtZEwYY3mjHmALTNxVWypClswT2AL5glswTyBLSp6K29WjgAAAABAhCMAAAAAkEQ4AgAAAABJhCMAAAAAkEQ4AgAAAABJhKNqa86YB5SXf6myywAAAACqDD4CW02dPp2joiJ2cQcAAACKsXIEAAAAACIcAQAAAIAkwhEAAAAASCIcAQAAAIAkwhEAAAAASGK3umrLx8fD6nde/iVlZ+VWUjUAAABA5SMcVVMJE9boj7P/C0PLpsQquxLrAQAAACobj9UBAAAAgAhHAAAAACCJcAQAAAAAkghHAAAAACCJcAQAAAAAkghHAAAAACCJcFQmnTp1UlJSUmWXIUmKj49XfHx8ZZcBAAAA/O3dcDhKTU2VwWDQrl27bui8zMxMJScn3/B5leW3335TcnKyjhw5UtmlAAAAALgFbtlHYE+dOqWUlBT5+fmpWbNmt+qyZbZ//36lpKSoRYsWatCggdWxVatWyc7OrpIqAwAAAFAR/vaP1eXm5t7yazo7O8vJyemWXxcAAABAxbnpcBQfH6/Y2Fjt2bNH8fHxCgsL07333qvZs2db+mzevFk9e/aUJI0ePVoGg0EGg0GpqamWPiaTSQMHDlRERITCw8P15JNP6ueff7a6VlJSkqKionTgwAElJCTIaDRq7NixkiSDwaDXXntNq1evVkxMjEJCQhQTE6P169dbjXH06FG9+uqr6tq1q0JDQ9WyZUslJiZaPT6XmpqqZ555RpI0YMAAS72bN2+WVPo7R4cOHVJiYqKio6MVFhamuLg4S/8rxzUYDNq2bZtee+01tWrVSuHh4XrmmWd05swZq75ff/21hgwZonbt2ikkJESdO3fWW2+9pcLCQtv/4wAAAACwWbk8Vnfu3DkNHjxY3bp1U/fu3bVq1Sq98cYbCgwMVPv27XX33Xdr5MiRmjp1qvr06aPIyEhJUkREhCQpLS1NQ4YMUVhYmBITE2U2m7Vw4UL1799fixcvVpMmTSzXunTpkhISEtSqVSslJSXJy8vLcuzHH3/UqlWr1K9fP7m7u+vDDz9UYmKivv32W91xxx2SpJ07dyo9PV0xMTGqW7eujh49qgULFmjAgAFavny53NzcFB0drSeeeELvv/++hg0bpsaNG0uS7r777lLv/9SpU4qLi9PFixcVHx8vDw8PLV68WAkJCZozZ45atmxp1X/s2LHy9vbW8OHDdeTIEb3//vsaN26cpk2bZunz2Wefyd3dXQMHDpS7u7t++OEHTZ8+XTk5OXrppZfK4b8aAAAAgCuVSzg6ceKEpkyZogcffFCS9Nhjj6lTp05asmSJ2rdvrzvvvFPt27fX1KlTFR4ertjYWMu5RUVFevXVV9WuXTvNnDnT0v7YY4+pe/fueuuttzR16lRLe25urh5++GGNGDGiRB2//fabVqxYIX9/f0lSy5YtFRsbq+XLl6t///6SpA4dOqhbt25W53Xs2FF9+vTR6tWr1bNnT/n7+6tFixZ6//331aZNmxLh5q/eeecdnTp1SgsXLlR4eLil/h49emjSpElWK2SSVKtWLb377ruW95aKior04YcfKjs7W56enpKkKVOmyNXV1XJOXFycXnnlFS1YsEAjR46Us7PzNWsqC19fz3IfE39vzAnYgnkCWzBPYAvmCWxRkfOkXMKRp6enYmJiLL+dnZ3VvHlzHT58+Lrn7t69WwcPHtTw4cNLPFoWGRmpLVu2lDinb9++pY7Vrl07SzCSpKCgIHl4eFjVcWXgKCgoUE5OjgICAuTl5aWMjAzL4383Yt26dTIajZZgJEleXl565JFH9M477ygzM1O+vr5W9V+5oUNUVJTee+89HT16VEFBQSXqzMnJ0cWLFxUVFaWFCxfq999/t/QrT5mZ2eU+Jv6+fH09mRO4LuYJbME8gS2YJ7DFzc4Te3s7+fh4XPV4uYSjevXqldi9rWbNmvr111+ve+6BAwckSc8//3ypx+3trV+LcnZ2Vp06dUrtW79+/RJtNWvWVFZWluV3Xl6eZs2apdTUVJ08eVJms9lyLDu7bH/Rx44dszwieKXix/GOHTtmFY7q1atn1a/40cAr69y7d6+mTZumH374QTk5OVb9y1onAAAAgKsrl3D01wBzI4rDyejRoxUYGHjd/i4uLjdcx5UBaPz48UpNTdUTTzyh8PBweXp6ys7OTiNHjrTqV5EcHByuWWdWVpb69+8vDw8PJSYmKiAgQC4uLvrll1/0xhtvqKio6JbUCQAAAFQnt+w7R1f7LlDxY3BeXl5q06ZNhddR/F7RlbvN5efn39RqTP369bV///4S7cVtpa1oXcuWLVt07tw5paSkKDo62tLOB2kBAACAinPLvnPk5uYmyfrRMUkKDg6Wv7+/5s6dW+o3i/76HtLNKm3V5sMPPyyxRba7u7sk2x5ha9++vdLT07Vjxw5LW3Z2tlJTUxUcHGz1SJ0tilfArlzJunjxoj7++OMbGgcAAACA7W7ZypGfn5+8vb31ySefqEaNGnJ3d1doaKj8/f01fvx4DRkyRA899JB69uyp2rVr68SJE9q0aZMCAgL03//+t9zq6NChg5YuXSoPDw81adJE27ZtU1pamry9va36BQUFydHRUbNnz1Z2dracnZ3VqlUr+fj4lBhzyJAhWr58uQYPHmy1lffZs2c1ZcqUG67RaDSqZs2aSkpKUnx8vOzs7LR06dJb9tgfAAAAUB3dspUjR0dHTZo0SY6Ojnr11Vc1atQo/fjjj5Kk1q1b65NPPlFgYKA+/PBDjR8/XkuXLpW/v/9Vd6Yrq5dfflmxsbFatmyZXn/9df3xxx+aN2+eatSoYdWvVq1aGjdunE6fPq2XX35Zo0aN0r59+0od884779SCBQvUsmVLvf/++5o6daq8vLxK/caRLe644w7NnDlTvr6+mjZtmubMmaM2bdrohRdeKNM9AwAAALg+OzPLEdVSwoQ1+uPs/x5jXDYllu0zYYUtVWEL5glswTyBLZgnsEVFb+V9y1aOAAAAAKAqIxwBAAAAgAhHAAAAACCJcAQAAAAAkghHAAAAACCJcAQAAAAAkm7hR2BRtcwZ84DV77z8S5VUCQAAAFA1EI6qqdOnc1RUxCeuAAAAgGI8VgcAAAAAIhwBAAAAgCTCEQAAAABIIhwBAAAAgCTCEQAAAABIIhxVWz4+HvL0cqvsMgAAAIAqg3BUTSVMWCNXF3ZyBwAAAIoRjgAAAABAhCMAAAAAkEQ4AgAAAABJhCMAAAAAkEQ4AgAAAABJhCMAAAAAkEQ4qnBHjhyRwWBQampqZZcCAAAA4BoIR6VITU2VwWDQrl27rNrPnj2rhx9+WEajUVu3bi3z+OvXr1dycvLNlgkAAACgHBGObHTu3DkNHDhQhw4d0qxZsxQVFVXmsTZs2KCUlJRyrA4AAADAzSIc2SArK0uDBg3S/v37NWPGDLVo0aKySypVbm5uZZcAAAAA/G0Rjq4jJydHCQkJ2rdvn95++221bt1akhQfH6/4+PgS/ZOSktSpU6erjpeUlKQPPvhAkmQwGCz/k6TNmzfLYDBo8+bNVueU9t5SUlKSoqKidODAASUkJMhoNGrs2LE3fb8AAABAdeVY2QVUZRcuXFBCQoJ2796tt956S23btr3pMfv06aPMzExt3LhRkydPvqmxLl26pISEBLVq1UpJSUny8vK66foAAACA6opwdA0vvvii/vjjD6WkpOi+++4rlzGNRqMaN26sjRs3KjY29qbGys3N1cMPP6wRI0aUeQxfX8+bqgG3N+YHbME8gS2YJ7AF8wS2qMh5Qji6hlOnTsnV1VV169at7FKuqm/fvjd1fmZmdjlVgtuNr68n8wPXxTyBLZgnsAXzBLa42Xlib28nHx+Pqx8v88jVwLhx42Rvb6/Bgwfr8OHDlV1OCc7OzqpTp05llwEAAADcFghH12AwGDRz5kzl5ORo4MCByszMvO45hYWFZb6enZ1dqe1FRUWltru4uJT5WgAAAACsEY6uIyIiQtOnT9eJEyeUkJCgrKwsSVLNmjUtf77SsWPHrjvm1UJQ8YYK2dnWS4VHjx690bIBAAAA3CDCkQ3uu+8+vf7669qzZ4+GDRumvLw8+fv76/fff9eZM2cs/Xbv3i2TyXTd8dzc3CSpRLjy8/OTg4ODfvzxR6v2BQsWlMNdAAAAALgWNmSw0YMPPqjz589r3LhxSkxM1L/+9S+99957SkhI0GOPPabTp0/rk08+UZMmTXThwoVrjhUSEiJJmjBhgtq1aycHBwfFxMTI09NT3bp100cffSQ7Ozv5+/vru+++0+nTp2/FLQIAAADVGuHoBjz++OM6e/askpOT5eXlpUmTJmn69OmaOHGimjRposmTJ+vLL7/Uli1brjnO/fffrwEDBujLL7/UF198IbPZrJiYGEnSmDFjdOnSJX3yySdydnZWt27d9OKLL+rBBx+8FbcIAAAAVFt2ZrPZXNlF4NZLmLBGc8Y8wJaZuCq2VIUtmCewBfMEtmCewBZs5Q0AAAAAtwDhCAAAAABEOAIAAAAASYQjAAAAAJBEOAIAAAAASYSjamvOmAeUl3+psssAAAAAqgy+c1RNnT6do6IidnEHAAAAirFyBAAAAAAiHAEAAACAJMIRAAAAAEgiHAEAAACAJMIRAAAAAEgiHFVbPj4e8vRyq+wyAAAAgCqDcFRNJUxYI1cXdnIHAAAAihGOAAAAAECEIwAAAACQRDgCAAAAAEmEIwAAAACQRDgCAAAAAEmEIwAAAACQRDgCAAAAAElSlfnQTWpqqkaPHm357eLiIj8/P91///0aOnSoPD09K7E6AAAAALe7KhOOio0cOVL16tVTbm6u0tLSNHv2bG3ZskULFy6UnZ1dZZcHAAAA4DZV5cJR+/bt1axZM0lS3759lZiYqNWrVys9PV0RERGVXN3/5Obmys3NrbLLAAAAAFBOqvw7Ry1btpQkHTp0SP/3f/+nXr16KTIyUuHh4erXr59++OEHq/5HjhyRwWDQe++9pwULFqhz584KCQnRo48+qh07dpQY//vvv1ffvn0VFham6OhoJSYm6vDhw1Z9kpKSFBUVpQMHDighIUFGo1Fjx46VJG3dulWJiYnq0KGDQkJC1L59e/3nP/9RXl5eqWMcPnxYgwcPVnh4uDp27KjU1FRJ0vbt29W3b1+Fhoaqa9eu2rRpk9X5R48e1auvvqquXbsqNDRULVu2VGJioo4cOXJzf8EAAAAAJFXBlaO/Kg4qzs7OWrRokR588EH17t1bFy5c0OLFizV48GAtWrTIstpUbOnSpfrzzz/Vp08f2dnZ6d1339Xw4cP19ddfy8nJSZKUlpamp556So0aNdKIESOUk5OjDz74QHFxcfriiy9Uq1Yty3iXLl1SQkKCWrVqpaSkJHl5eUmSVq1apby8PMXFxcnb21s7duzQRx99pBMnTmj69OlWNV26dElPPfWUWrdurY4dO2rJkiX697//LScnJ02aNEm9e/dW9+7dNW/ePI0YMULr1q1TjRo1JEk7d+5Uenq6YmJiVLduXR09elQLFizQgAEDtHz5claxAAAAgJtlriKWLFliDgwMNP/www/m06dPm48ePWr+9NNPzc2bNze3bt3afOHCBXN+fr7VOefPnze3adPGPHr0aEvb4cOHzYGBgeZWrVqZs7KyLO1ff/21OTAw0Lx27VpLW2xsrLlt27bm8+fPW9rS09PNgYGB5okTJ1raXnrpJXNgYKB52rRpJerOzc0t0TZr1iyzwWAwHz16tMQYs2fPtrSdOHHC3KxZM7PBYDCnpaVZ2jds2GAODAw0f/bZZ9e8TnGtV/az1aDxq2/4HAAAAOB2VuVWjgYMGGD1u3Hjxpo0aZLc3d0tbUVFRcrKylJRUZFCQkKUkZFRYpyYmBirHe6ioqIk/W8l6o8//tCuXbs0bNgwyyqQJIWHhys8PFzfffedkpKSrMbs27dvieu4urpa/vznn38qLy9PRqNRZrNZGRkZql+/vlX/3r17W/5cp04d1a1bV5LUunVrS3tYWJgkWT0yd+V1CgoKlJOTo4CAAHl5eSkjI0M9e/YsUZstMjOzy3Qebn++vp7MD1wX8wS2YJ7AFswT2OJm54m9vZ18fDyuerzKhaOxY8cqICBADg4Oql27tu666y7Lsc8++0xz587V/v37VVBQYGlv0KBBiXH+Gkpq1qwpScrKypIkHTt2TJKsxi/WuHFjLV++3KrN2dlZderUKdH32LFjmj59utauXavz589bHcvJybH67e7ubqmjmKenp+XRuSvbrqxVkvLy8jRr1iylpqbq5MmTMpvNlmPZ2fxDAgAAANysKheOwsLCSrw/JF1+hygpKUmdO3dWQkKCfHx85ODgoFmzZpXYQEGS7O1L32viylBxI1xcXEq0FRYWauDAgTp//rwGDx6sxo0by93dXSdPnlRSUpKKioqs+js4OJQ69tXar6x1/PjxSk1N1RNPPKHw8HB5enrKzs5OI0eOLPM9AQAAAPifKheOrmb16tXy9/dXSkqK1feO/rrpga2KV5b2799f4tj+/ftLrDyVZs+ePTpw4IAmTZpk9VjbX3eaKw+rV69Wz549rR71y8/PZ9UIAAAAKCdVfivvYsWrK1eukmzfvl3btm0r03i1a9dWs2bNtGTJEquAsWPHDqWnp6tDhw7XHaN4derKmsxmsz744IMy1XQtpa0uffjhhyosLCz3awEAAADV0d9m5ahDhw5as2aNnnnmGXXo0EFHjhzRJ598oiZNmujPP/8s05gvvviiBg8erLi4OD366KOWrbx9fX01ZMiQ657fuHFjBQQEaNKkSTp58qQ8PDy0evVqq3eFykuHDh20dOlSeXh4qEmTJtq2bZvS0tLk7e1d7tcCAAAAqqO/TTjq1auXTp06pYULF2rjxo1q0qSJ/vvf/2rVqlXasmVLmcZs06aN3n33XU2fPl1Tp06Vs7Oz2rRpoxdeeMHqG0dX4+TkpJkzZ2rChAmaNWuWXFxc1KVLFz3++OOKjY0tU01X8/LLL8ve3l7Lli1Tfn6+IiIiNG/ePA0ePLhcrwMAAABUV3Zm3uavlhImrNGcMQ+wZSauii1VYQvmCWzBPIEtmCewRUVv5f23eecIAAAAACoS4QgAAAAARDgCAAAAAEmEIwAAAACQRDgCAAAAAEmEIwAAAACQRDiqtuaMeUB5+ZcquwwAAACgyvjbfAQW5ev06RwVFfGJKwAAAKAYK0cAAAAAIMIRAAAAAEgiHAEAAACAJMIRAAAAAEgiHAEAAACAJMnObDazZRkAAACACpeXf0nZWbllPt/X11OZmdllPt/e3k4+Ph5XPc5W3tVUwoQ1+uNs2ScmAAAAcKOWTYlV2aNNxeOxOgAAAAAQ4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS4QgAAAAAJBGOAAAAAEAS3zmy2a+//qq33npLO3fu1KlTp+Tt7a0mTZqoU6dOio+Pv+X1rF+/Xtu3b9fw4cNv+bUBAACA2xErRzYwmUx69NFHtXv3bvXu3VuvvPKKevfuLXt7e33wwQeVUtOGDRuUkpJSKdcGAAAAbkesHNlg5syZqlmzphYvXiwvLy+rY6dPn76ltfz5559yd3e/pdcEAAAAqgPCkQ0OHTqkwMDAEsFIknx8fCx/NhgMGjBggIKDgzVjxgwdO2/eDrQAACAASURBVHZMQUFBGjNmjMLCwqzO++WXX/Tmm2/KZDJJkiIiIvTCCy8oKCjI0ic5OVkpKSlauXKlpk+frg0bNuiee+6Rn5+fPvvsM8s1i/3666/let8AAABAdUI4soGfn5+2b9+uffv2qUmTJtfs+8MPP2j58uXq37+/HB0dNX/+fA0cOFCff/65AgICJEl79+5V//795eXlpaFDh0qSFixYoH79+mnRokW6++67rcYcPny47r77bj3//PNydHRUkyZNlJmZqY0bN2ry5MkVc9MAAABANUM4ssGgQYP01FNP6eGHH1ZoaKiioqLUunVrtWjRQk5OTlZ99+7dq88//9yyAtStWzd1795dM2bM0MSJEyVJ06ZNU2FhoT7++GP5+flJkh588EF1795d06ZNU3JystWYwcHBJUJQ48aNtXHjRsXGxlbUbQMAAADlztfXs1LPvxbCkQ3atm2rTz75RO+88442btyo9PR0zZ49W3feeacmTJigjh07WvpGRkZaPRoXEBCge++9V+vXr5ckFRYWatOmTerSpYslGElSgwYN1KVLF3377bcqLCyUg4OD5Vjfvn1vwV0CAAAAFS8zM7vM5/r6et7U+fb2dvLx8bj68TKPXM2EhoYqJSVFW7Zs0aJFizR06FBlZ2dr+PDh+u233yz9GjZsWOLchg0b6tSpU8rPz9eZM2eUm5uru+66q0S/xo0b688//9TZs2et2hs0aFD+NwQAAADACuHoBjk7Oys0NFSjRo3Sq6++qoKCAq1cubJCr+nq6lqh4wMAAAAgHN2UkJAQSdIff/xhaTt48GCJfgcPHpSPj49cXFxUq1Ytubm5af/+/SX67d+/X+7u7rrjjjuue207O7ubqBwAAADAXxGObPDDDz/IbDaXaF+3bp2ky4/DFfvpp5+0e/duy+9Dhw5p48aNuu+++yRJDg4Oatu2rb766isdO3bM0u/YsWP66quv1K5dO6v3ja7Gzc1NkpSVlVW2mwIAAABghQ0ZbDBhwgTl5uaqS5cuaty4sQoKCmQymbRy5Ur5+fmpV69elr5NmzbVoEGDFB8fLwcHB82fP19OTk4aNmyYpc9zzz2ntLQ09evXT3FxcZIub+Xt4OCg5557zqaailetJkyYYAlUMTEx5XjXAAAAQPVCOLLBiy++qFWrVmndunVauHChCgoKVL9+ffXr109PP/201cdhW7VqpeDgYL399ts6fvy4DAaDpk2bpkaNGln6NG3aVB999JGmTJmimTNnSrr8Edjnn3++xDeOrub+++/XgAED9OWXX+qLL76Q2WwmHAEAAAA3wc5c2vNiKBODwaABAwbo5ZdfruxSrithwhr9cTa3sssAAABANbJsSixbeQMAAABAVUc4AgAAAAARjgAAAABAEhsylKtff/21sksAAAAAUEasHAEAAACA2K0OAAAAwC2Sl39J2Vll3zG5oner47G6aur06RwVFZGLcXU3+48PqgfmCWzBPIEtmCeoCnisDgAAAABEOAIAAAAASYQjAAAAAJBEOAIAAAAASYQjAAAAAJBEOKq2PDxcK7sEAAAAoEohHFVTLi7s4g4AAABciXAEAAAAACIcAQAAAIAkwhEAAAAASCIcAQAAAIAkwhEAAAAASCIcAQAAAIAkwtHfhsFg0GuvvVbZZQAAAAC3LcLRTVqxYoUMBoPWrl1b4ljnzp1lMBiUkZFh1X7x4kWFhoZqxIgRt6pMAAAAANdBOLpJkZGRkqT09HSr9szMTB0+fFiOjo4ljmVkZCg/P99yLgAAAIDKRzi6SXXq1JGfn59MJpNVu8lkkouLizp16lTqMUmEIwAAAKAKIRyVg4iICO3cuVMXL160tJlMJgUHBys6OrpEOEpPT5e7u7uCgoJUVFSkOXPmqHv37goJCVG7du00fvx4XbhwodRrff755+ratauaN2+u3r17a/v27RV6bwAAAEB1QTgqBxEREcrPz7d6t8hkMsloNMpoNOrYsWM6ceKE1bHw8HA5ODjo5Zdf1tSpU9WiRQuNGTNGDz30kD799FP985//lNlstrrODz/8oMmTJys2NlbDhw/XH3/8oYEDB+rQoUO37F4BAACA25VjZRdwO4iIiJD0v9CTl5enXbt2aejQoQoKCpKrq6tMJpN69OihQ4cO6dSpU4qLi9PWrVuVmpqq6dOnq2vXrpbxmjdvrpEjR2rDhg267777LO179+7V559/rqCgIElSt27d1L17d82YMUMTJ0684bp9fT1v8s5xu2OOwBbME9iCeQJbME9gi4qcJ4SjchAYGChPT0+ZTCYNGjRIO3bsUEFBgYxGo5ycnBQSEmIJR1e+b7Rq1Sp5e3srOjpaZ86csYwXFRUlBwcHbdmyxSocRUZGWoKRJAUEBOjee+/V+vXry1R3ZmZ2Ge8Y1YGvrydzBNfFPIEtmCewBfMEtrjZeWJvbycfH4+rHicclQN7e3uFh4dbgo/JZFLDhg3l4+MjSTIajUpLS7Mcc3BwUGhoqObOnatz586pdevWpY57ZWCSpIYNG5bo07BhQ3377bfKz8+Xi4tLed4WAAAAUK0QjspJZGSkNmzYoIMHD1reNyoWHh6uuXPn6sKFC0pPT1dQUJBq1KihoqIi+fr6avLkyaWOWbt27VtVPgAAAFDtEY7KSfF7R1u3btW2bds0atQoq2OFhYXauHGj9u3bp/79+0u6/Fjc5s2bFRUVJWdn5+te4+DBg6W2+fj4sGoEAAAA3CR2qysnoaGhcnJy0qJFi3T+/HmrlaNatWopICBA8+bNU1FRkSVIde3aVQUFBXrnnXdKjHfx4kXl5ORYtf3000/avXu35fehQ4e0ceNGq/eSAAAAAJQNK0flxM3NTc2aNVN6ero8PT3VtGlTq+NGo1FLly6V9L+Pv7Zq1Uq9e/dWcnKyfv75Z7Vu3Vr29vY6cOCAVq5cqTfeeENt2rSxjNG0aVMNGjRI8fHxcnBw0Pz58+Xk5KRhw4bduhsFAAAAblOEo3IUERGhHTt2KCwsTPb21oty4eHhWrp0qfz9/a3eJRo/fryCg4P16aefasqUKXJ2dlaDBg3Uu3dvq53ppMthKjg4WG+//baOHz8ug8GgadOmqVGjRrfi9gAAAIDbmp35r18aRbXBdpm4FrZUhS2YJ7AF8wS2YJ7AFhW9lTfvHAEAAACACEcAAAAAIIlwBAAAAACSCEcAAAAAIIlwBAAAAACSCEcAAAAAIIlwVG3l51+q7BIAAACAKoVwVE3l5ORVdgkAAABAlUI4AgAAAAARjgAAAABAEuEIAAAAACQRjgAAAABAEuEIAAAAACRJjpVdACqHj4+HJCkv/5Kys3IruRoAAACg8rFyVE0lTFijh/61VK4u5GMAAABAIhwBAAAAgCTCEQAAAABIIhwBAAAAgCTCEQAAAABIIhwBAAAAgCTCEQAAAABIug3CUXJysgwGQ2WXYWEwGPTaa6+V65i///67nnjiCUVERMhgMGjz5s3lOj4AAACAKhCOVqxYIYPBoLVr15Y41rlzZxkMBmVkZFi1X7x4UaGhoRoxYsQNXSs/P1/Jycl/u3CRlJSk/fv361//+pcmT56su+++W8uXL9d7771X2aUBAAAAt41KD0eRkZGSpPT0dKv2zMxMHT58WI6OjiWOZWRkKD8/X5GRkXr66ae1Y8cOm66Vn5+vlJQUbdmypXyKvwXy8vK0fft2/eMf/9Djjz+u2NhY3XnnnVq+fLk++OCDyi4PAAAAuG1UejiqU6eO/Pz8ZDKZrNpNJpNcXFzUqVOnUo9Jl4OVo6OjXFxcrnmNwsJCXbx4sXwLv0XOnDkjSfLw8KjkSgAAAIDbW6WHI0mKiIjQzp07rQKMyWRScHCwoqOjS4Sj9PR0ubu7KygoqNR3jorf+/n888/VrVs3NW/eXF9++aWio6MlSSkpKTIYDDIYDEpOTpZ0eaVq9OjRuu+++xQSEqJ27drp6aef1pEjR6zG/uyzz9SrVy+FhYWpRYsWeuKJJ7R169YS97R69WrFxMQoJCREMTExWr9+vdXxo0eP6tVXX1XXrl0VGhqqli1bKjEx0ep6ycnJ6tixoyRp4sSJMhgM6tSpk+Lj4/XNN9/o6NGjlvvo1KnTjf61AwAAALiCY2UXIF0OR8uWLVNGRobCw8MlXQ5H0dHRMhqNeu2113TixAnVrVvXciw8PFwODg5XHXPTpk1auXKl+vXrJy8vL91zzz0aN26cXnnlFXXp0kVdunSRJEuwGj58uI4cOaK4uDjVrl1bp06dUlpamo4fP64GDRpIkqZNm6YZM2YoKipKzz33nOzs7JSenq6tW7cqKirKcu0ff/xRq1atUr9+/eTu7q4PP/xQiYmJ+vbbb3XHHXdIknbu3Kn09HTFxMSobt26Onr0qBYsWKABAwZo+fLlcnNzU5cuXeTp6amJEyfq4YcfVrt27VSjRg25ubnpwoULOnbsmEaPHi1JqlGjRjn/VwEAAACqlyoTjqT/hZ68vDzt2rVLQ4cOVVBQkFxdXWUymdSjRw8dOnRIp06dUlxc3DXHPHDggJYvX6677rrL0la/fn298sorMhgMio2NtbRnZWUpPT1d06ZNU/fu3S3tTz/9tNV4s2bNUrdu3TR16lTZ219edHvyySdlNputrv3bb79pxYoV8vf3lyS1bNlSsbGxWr58ufr37y9J6tChg7p162Z1XseOHdWnTx+tXr1aPXv2VFBQkDw8PDRx4kQFBwdb1Tx//nydO3fOqq2sfH09b3oM3J6YG7AF8wS2YJ7AFswT2KIi50mVCEeBgYHy9PSUyWTSoEGDtGPHDhUUFMhoNMrJyUkhISGWcHTl+0bX0qpVK6tgdC2urq5ycnLShg0b1L59e7m7u5fo8/XXX6uoqEjPPPOMJRgVs7Ozs/rdrl07SzCSZAk5hw8ftrpmsYKCAuXk5CggIEBeXl7KyMhQz549baq9PGRmZt+ya+Hvw9fXk7mB62KewBbME9iCeQJb3Ow8sbe3k4/P1d/lrxLhyN7eXuHh4ZbgYzKZ1LBhQ/n4+EiSjEaj0tLSLMccHBwUGhp6zTGLH4WzhbOzs55//nlNmjRJX375pYxGozp27KiHH35YtWrVkiQdPnxYDg4Oaty48XXHq1+/fom2mjVrKisry/I7Ly9Ps2bNUmpqqk6ePGm1+pSdzT8MAAAAwK1WJTZkkC6vBJ0+fVoHDx6UyWSS0Wi0HAsPD9fu3bt14cIFpaenKygo6Lrv2FxvB7u/evLJJ7VmzRqNHDlSjo6OeuONN9S9e3ft3r37hu/lrytLxa4MQOPHj9fMmTPVvXt3TZs2TXPnztW8efPk7e1d4jE9AAAAABWvyoSj4veOtm7dqm3btlmFo4iICBUWFmrjxo3at2/fdR+pu5q/Pv72V/7+/ho4cKDmzJmjlStX6uLFi5ozZ44kKSAgQIWFhfr999/LdO2/Kn6vKCkpSd26dVPbtm0VGRlp86rR9e4FAAAAwI2pMuEoNDRUTk5OWrRokc6fP28VjmrVqqWAgADNmzdPRUVFliB1o1xcXGRnZ2f1eJsk5ebmKi8vz6rNz89Pnp6eys/PlyTdf//9sre3V0pKioqKiqz6lmWlp7Sd9j788EMVFhbadL6bmxuP3wEAAADlqEq8cyRd/j/7zZo1U3p6ujw9PdW0aVOr40ajUUuXLpV0/c0YrsbZ2VmBgYFauXKlGjVqJG9vbzVt2lSFhYV68skn1bVrVzVp0kROTk76+uuvdfLkScXExEiSGjVqpKeeekqzZs1SfHy8OnfuLAcHB23btk2BgYEaNmzYDdXSoUMHLV26VB4eHmrSpIm2bdumtLQ0eXt723R+SEiIli1bpokTJ6p58+Zyd3fnW0cAAADATagy4Ui6/Pjcjh07FBYWVuK9nfDwcC1dulT+/v6qXbt2ma8xfvx4jRs3Tq+//rouXryoZ599Vv3791dMTIy+//57LVu2zLLxwrRp09S1a1fLuaNGjVKDBg00f/58vfnmm3J3d1ezZs0sH5e9ES+//LLs7e21bNky5efnKyIiQvPmzdPgwYNtOr9Pnz765Zdf9Nlnn+m9996Tn58f4QgAAAC4CXZm3v6vlhImrNEfZ3O1bEos22aiVGypClswT2AL5glswTyBLSp6K+8q884RAAAAAFQmwhEAAAAAiHAEAAAAAJIIRwAAAAAgiXAEAAAAAJKq2FbeuHXmjHlAkpSXf6mSKwEAAACqBsJRNXX6dI6KitjFHQAAACjGY3UAAAAAIMIRAAAAAEgiHAEAAACAJMIRAAAAAEgiHAEAAACAJMJRteXj4yFPL7fKLgMAAACoMghH1VTChDVydWEndwAAAKAY4QgAAAAARDgCAAAAAEmEIwAAAACQRDgCAAAAAEmEIwAAAACQRDgCAAAAAEm3IBwlJyfLYDBU9GWqjOL7zcrKKtdxZ82apU6dOqlZs2aKj48v17EBAAAA2BCOVqxYIYPBoLVr15Y41rlzZxkMBmVkZFi1X7x4UaGhoRoxYkT5VVqNrV+/Xm+++aZatmypiRMnatiwYcrMzFRycrJ27dpV2eUBAAAAt4XrhqPIyEhJUnp6ulV7ZmamDh8+LEdHxxLHMjIylJ+fr8jISD399NPasWNHOZZc/WzZskWOjo4aP368evbsqbZt2+rUqVNKSUkhHAEAAADl5LrhqE6dOvLz85PJZLJqN5lMcnFxUadOnUo9Jl0OVo6OjnJxcSnHkquf06dPy83NTY6OjpVdCgAAAHDbsumdo4iICO3cuVMXL160tJlMJgUHBys6OrpEOEpPT5e7u7uCgoJKfedo06ZNiouLU1RUlIxGo7p27ao333zTcrygoEApKSl64IEH1Lx5c7Vs2VJxcXHatGmT1Tgmk0kDBw5URESEwsPD9eSTT+rnn3+26pOUlKSoqCgdP35cw4YNk9FoVKtWrTRp0iQVFhZa9V2+fLl69eolo9GoiIgIPfTQQ3r//fet+hw/flyjR49Wu3bt1Lx5c3Xp0kUTJkwo8Xd2/vx5vfjii4qMjFRkZKRGjx6t3Nxcqz5LlizRgAED1Lp1a4WEhKhHjx76+OOPrfoYDAalpqYqOztbBoPB8rtnz56SpNGjR1u1AwAAACgbm5YiIiIitGzZMmVkZCg8PFzS5WASHR0to9Go1157TSdOnFDdunUtx8LDw+Xg4FBirL1792ro0KGKiIjQyJEjZW9vr4MHD+qnn36y9ElJSdGcOXPUr18/NW3aVNnZ2dq5c6d++eUXtW3bVpKUlpamIUOGKCwsTImJiTKbzVq4cKH69++vxYsXq0mTJpbxLl26pEGDBikiIkIvvvii0tLSNHfuXPn7+6tfv36SLge2UaNG6YEHHtA//vEPFRYWat++fTKZTHriiSckSSdPnlTv3r114cIF9enTR3fddZeOHTumFStWaMyYMVb3mZiYKH9/f/3rX/9SRkaGFi1apFq1aumFF16w9FmwYIGaNm2qTp06ydHRUd9++63Gjh0rs9msxx9/XJI0efJkffrpp/rll180duxYSVKjRo00cuRITZ06VX369LE8+hgREWHLf04AAAAApbA5HEn/Cz15eXnatWuXhg4dqqCgILm6uspkMqlHjx46dOiQTp06pbi4uFLH2rRpk1xcXDRv3rxSw5Mkfffdd+rdu7f+/e9/l3q8qKhIr776qtq1a6eZM2da2h977DF1795db731lqZOnWppz83NVc+ePTV06FBJUlxcnB555BEtXrzYEo6+++47NW3aVMnJyVf9e3jjjTd05swZLVmyRM2aNbO0P/fccyX6Nm/eXOPGjbP8PnfunBYvXmwVjj766CO5urpafvfv318JCQmaN2+eJRzFxsbq+++/16+//qrY2FhLX2dnZ02dOlXh4eFW7TfK19ezzOfi9sf8gC2YJ7AF8wS2YJ7AFhU5T2wKR4GBgfL09JTJZNKgQYO0Y8cOFRQUyGg0ysnJSSEhIZZwdOX7RqXx8vJSbm6uNmzYoA4dOly1z/bt261Wo660e/duHTx4UMOHD9eZM2esjkVGRmrLli0lzunTp0+Jfl988YXVNY8fP67t27crLCysxPlFRUX65ptv1LlzZ6tgJEl2dnYl+vft29fqd1RUlL766ivl5OTIw8NDkqyCUXZ2tgoKCtSiRQtt3LhR2dnZ8vSs+H8gMjOzK/wa+Hvy9fVkfuC6mCewBfMEtmCewBY3O0/s7e3k4+Nx1eM2hSN7e3uFh4dbgo/JZFLDhg3l4+MjSTIajUpLS7Mcc3BwUGhoaKlj9ejRQ4sWLdLQoUPl6+urNm3aqEuXLurcubMlZCQmJuqf//ynOnTooHvuuUf33nuvHnroIcujcgcOHJAkPf/881et90ru7u7y9va2aqtZs6bOnz9v+d2vXz+tXLlS//jHP9SgQQO1bdtW3bp1U5s2bSRJZ86c0YULF9S0aVNb/spUr149q99eXl6SLr+LVByOfvrpJyUnJ2vbtm0l3ke6VeEIAAAAwGU2b38WGRmpDRs26ODBgzKZTDIajZZj4eHhmjt3ri5cuKD09HQFBQWpRo0apY7j6uqq+fPna/PmzVq3bp02bNigpUuXqm3btpo9e7YcHBwUHR2tr776SmvXrtWmTZu0YMECvfvuuxo3bpweffRRmc1mSZc3IwgMDLxu7Vd7fO9KPj4++vzzz7Vp0yatX79e69ev18KFC/Xoo4/qP//5j41/S9e/ZnHthw4d0pNPPqnGjRsrKSlJ9erVk5OTk9atW6f33ntPRUVFN3xNAAAAAGVnczgqfu9o69at2rZtm0aNGmV1rLCwUBs3btS+ffvUv3//a45lb2+v1q1bq3Xr1kpKStLs2bP1xhtvaMuWLWrdurUkydvbW7169VKvXr30559/Kj4+XtOnT9ejjz4qf39/SZdXY4pXdsqDs7OzOnbsqI4dO8psNmv8+PGaP3++hg0bpgYNGqhGjRrau3dvuVxr7dq1unjxombMmKH69etb2jdv3mzT+aU9ygcAAACg7GzayluSQkND5eTkpEWLFun8+fNWK0e1atVSQECA5s2bp6Kiomvumnb27NkSbcXv8OTn55fax93dXY0aNbIcDw4Olr+/v+bOnVvicTRJJd5DssVfr2lnZ2fZgjw/P1/29va6//779fXXXysjI8Oqb/Fq0I0oXlm68tzs7GwtWbLEpvPd3NwkSVlZWTd8bQAAAAAl2bxy5ObmpmbNmik9PV2enp4l3r0xGo1aunSppKtvxiBJb7/9trZu3ar77rtPDRo00JkzZ/Txxx+rbt26lvNiYmIUHR2tkJAQeXt76+eff9aKFSssO7g5ODho/PjxGjJkiB566CH17NlTtWvX1okTJ7Rp0yYFBATov//97w39Rfx/e/ceVHWd/3H8CYgmIiAqjutd13MyBcH7BWcUlbA11NJEES0xolxDnSxd26lxqxWzpHUxrcwSdBRQNFxb8bZ/eMtyXNH1soqCsCrhBVdAQOT7+8Ph/CTA4OABYV+PGWbic76fc97Ye875vvh8vx/effddbt++zaBBg2jTpg1Xr14lNjaWHj160K1bNwDmz5/PwYMHCQ4OJigoiC5dunD16lV27tzJrl27qvV6Q4cOxdHRkfDwcIKCgsjLyyM+Pp6WLVuSnZ39q/PbtWuHm5sbmzZtolmzZjg5OeHl5WVZVRMRERERkeqpcjiCB5fPpaSk0Lt373KbHnh7e7N9+3Y6dOiAh4dHpc/h5+fHf/7zH7Zu3cqtW7do0aIFAwYMYM6cOZYNCEJCQti3bx+HDh2iqKiI3/zmN0RERBAaGmp5nsGDB7Np0yaio6OJiYkhPz8fDw8PfHx8yu0UVxWBgYHExcWxceNG/vvf/9K6dWsCAgKYM2eO5Wdt27YtcXFxREVFkZiYSF5eHm3btq10171H6dq1K3/5y1+IiooiMjKSVq1aMWXKFNzd3SvdwvxhjRo1IjIykuXLl/P+++9TXFzMn//8Z4UjEREREREr2RnWXBMm9V7oB8msfddfW2ZKpbSlqlSF+kSqQn0iVaE+kaqw9VbeVb7nSEREREREpCFTOBIREREREUHhSEREREREBFA4EhERERERARSOREREREREAIUjERERERERQOHof9bad/0pKCyu6zJERERERJ4Y1fojsNJw3LiRS0mJ/sSViIiIiEgprRyJiIiIiIigcCQiIiIiIgIoHImIiIiIiAAKRyIiIiIiIoDCkYiIiIiICKBwJCIiIiIiAigciYiIiIiIAApHIiIiIiIigMKRiIiIiIgIoHAkIiIiIiICKByJiIiIiIgACkciIiIiIiIANKrrAqRu2Nvb1XUJUg+oT6Qq1CdSFeoTqQr1iVRFTfrk1+baGYZhWP3sIiIiIiIiDYQuqxMREREREUHhSEREREREBFA4EhERERERARSOREREREREAIUjERERERERQOFIREREREQEUDgSEREREREBFI5EREREREQAhSMRERERERFA4UhERERERARQOGowioqK+Pjjj/H19cXLy4uXXnqJw4cPV2luVlYWERER9OvXjz59+vDGG2+QkZFh44qlLljbJ8nJycydOxc/Pz969+5NQEAAkZGR3LlzpxaqltpWk/eTh7366quYzWY+/PBDG1Qpda2mfZKUlMTEiRPx9vZmwIABTJs2jZSUFBtWLHWhJn1y6NAhQkJCGDhwIP3792fy5Mns3LnTxhVLXfj5559Zvnw5ISEh+Pj4YDab+eGHH6o8PzU1ldDQUHx8fBgwYADvvPMON2/etKoWhaMGYuHChXz77bcEBgayePFi7O3tefXVVzl+/Pgj5+Xl5TF9+nSOHTtGeHg4b775JqdPn2b69Oncvn27lqqX2mJtn/zxj38kNTWVcePG8e677+Lr60tMTAxTpkyhsLCwlqqX2mJtnzzsH//4Bz/99JMNq5S6VpM+WbFiBQsXLqR79+4sxsUOBwAAEP5JREFUXryY2bNn06FDB7Kzs2uhcqlN1vbJ/v37mTlzJsXFxcyZM4eIiAjs7e2ZN28e8fHxtVS91JZLly7x5ZdfkpWVhdlsrtbca9euERwcTEZGBvPmzWPmzJns37+f0NBQ7t27V/1iDKn3Tpw4YZhMJmPdunWWsYKCAmPUqFHG1KlTHzn3iy++MMxms/Gvf/3LMnbhwgWjR48eRlRUlK1KljpQkz45cuRIubHExETDZDIZW7ZsedylSh2qSZ+UKiwsNPz9/Y2VK1caJpPJ+OCDD2xUrdSVmvTJsWPHDLPZbCQnJ9u4SqlrNemT0NBQw9fX1ygsLLSMFRYWGr6+vkZwcLCtSpY6cufOHePmzZuGYRjG7t27DZPJVOG5R0Xee+89w9vb27h27Zpl7ODBg4bJZDLi4+OrXYtWjhqAv//97zg6OjJp0iTLWJMmTZg4cSLHjh3j559/rnTurl278Pb25plnnrGMdevWjcGDB/P999/btG6pXTXpk4EDB5YbGzVqFPBgKVsajpr0San169dTUFBAaGioLUuVOlSTPlm/fj2enp6MHj2akpIS8vLyaqNkqQM16ZPc3FxcXV1p3LixZaxx48a4urrSpEkTm9Yttc/Z2ZkWLVpYNTc5ORk/Pz/atGljGRsyZAidO3e26lxW4agBOHPmDF26dKFZs2Zlxr28vDAMgzNnzlQ4r6SkhHPnztGrV69yj3l6epKWlsbdu3dtUrPUPmv7pDLXr18HsPrNTJ5MNe2T7OxsVq1axbx582jatKktS5U6VJM+OXz4MJ6ennz66af07duXPn364Ofnx3fffWfrsqWW1aRPBgwYwPnz54mKiuLy5ctcvnyZqKgo0tLSmDlzpq1Ll3oiKyuLGzduVHgu6+XlVe1zG4BGj6MwqVvZ2dll0nKp1q1bA1T6m5mcnByKioosx/1yrmEYZGdn07Fjx8dbsNQJa/ukMl9++SUODg74+/s/lvrkyVDTPvn000/p0qUL48aNs0l98mSwtk9u375NTk4Of/vb33BwcOCtt97Czc2NDRs2sGDBApo2bcro0aNtWrvUnpq8n4SHh3P58mVWr17N559/DoCTkxOrVq1i6NChtilY6p3SHqrsXPbGjRvcv38fBweHKj+nwlEDUFBQgKOjY7nx0mXnym6YLx1/eMn6l3MLCgoeV5lSx6ztk4okJSWRkJDAa6+9pvDcwNSkT1JSUti2bRsxMTHY2dnZrEape9b2SX5+PvDgl3NxcXH07t0bgNGjRzN69Giio6MVjhqQmryfNG7cmM6dOxMQEMDo0aO5f/8+cXFxzJ07l2+++QYvLy+b1S31R1XPZX+5evkoCkcNwFNPPVXhbhylDVPZtbml40VFRZXOfeqppx5XmVLHrO2TX/rpp59YvHgxw4cPJyIi4rHWKHXP2j4xDIMPP/wQf39/+vXrZ9Mape7V9HOnffv2lmAED05snn32WdavX09eXl61TmTkyVWTz50//elPnDx5koSEBOztH9wFMmbMGMaOHctHH33Epk2bbFO01Cu2OJfVPUcNQOvWrStcmi7dEtXDw6PCeW5ubjRu3LjCrVOzs7Oxs7OrcJlS6idr++RhZ8+e5fXXX8dsNrNixYpqLVNL/WBtn+zevZuUlBSmTJlCZmam5Qse3FidmZmplegGpKafO61atSr3WKtWrTAMg9zc3MdbrNQZa/ukqKiIhIQEhg8fbglGAI6OjgwbNoyTJ09SXFxsm6KlXintocrOZVu2bFntcxWFowbg6aef5tKlS+V2/Dlx4oTl8YrY29tjMpk4depUucdSUlLo1KmTbqhuQKztk1KXL19m1qxZuLu7s2bNGpycnGxWq9Qda/vkypUrlJSUMGPGDEaOHGn5Ati6dSsjR47k6NGjti1eak1NPnd69OhBVlZWuceuXbuGg4MDrq6uj79gqRPW9klOTg7FxcXcv3+/3GPFxcUUFxdjGMbjL1jqnTZt2uDu7l7puWyPHj2q/ZwKRw1AQEAA9+7dK/NH0YqKiti6dSt9+vSx3Ax55cqVctsuP/vss/zzn//k9OnTlrGLFy9y5MgRAgICaucHkFpRkz7Jzs5m5syZ2NnZsXbtWtzd3Wu1dqk91vaJn58f0dHR5b4ARowYQXR0ND179qzdH0ZspibvJwEBAVy9epWDBw9axnJzc/n+++/x8fHR5dwNiLV90rJlS1xcXNi9e3eZy/Ly8vLYv38/JpOpwnuZpOEr3bnwYf7+/uzbt6/ML10OHz5MWlqaVeeydoaid4MQERHB3r17mTFjBh07diQxMZFTp07x7bff0rdvXwBCQkI4evQo586ds8zLzc1lwoQJ3L17l1deeQUHBwe++eYbDMNg27Zt2qa5gbG2T8aNG8fZs2eZNWsWJpOpzHN27NgRHx+fWv05xLas7ZOKmM1mpk+fzuLFi2ujdKlF1vbJ3bt3eeGFF8jKyuLll1/GxcWFLVu2cOnSpTJzpWGwtk8+//xzoqKi6NmzJ4GBgZSUlJCQkEBqaiorVqzgueeeq6sfSWxk1apVwIO/n7hjxw5efPFF2rdvj4uLC9OmTQMe/CIOYN++fZZ5V69eZfz48bi5uTFt2jTy8/NZu3Ytbdu2JT4+vsLNGh5FGzI0EMuWLSMqKort27dz+/ZtzGYzX3zxxa9+yDg7OxMTE8NHH33EqlWrKCkpYeDAgSxevFjBqAGytk/Onj0LwFdffVXusQkTJigcNTDW9on8b7G2T5o2bcr69etZtmwZsbGxFBQU0LNnT9atW6cea4Cs7ZPXX3+d9u3bs379eqKjoykqKsJsNvPXv/5VOxo2UJ999lmZ77ds2QJAu3btLOGoIm3btiU2NpalS5fyySef4OjoyPDhw1m0aFG1gxFo5UhERERERATQPUciIiIiIiKAwpGIiIiIiAigcCQiIiIiIgIoHImIiIiIiAAKRyIiIiIiIoDCkYiIiIiICKBwJCIiIiIiAigciYiIWO3MmTPMmDGD/v37YzabWblyJZmZmZb/roqFCxdiNpttXKmIiFRFo7ouQEREpLru3r3L5s2bSU5O5sKFC+Tl5eHq6krPnj0ZM2YMgYGBNGpk24+44uJi5syZQ3FxMRERETRv3lwhR0SknlM4EhGReiU9PZ2wsDDS0tIYMmQIYWFhtGjRghs3bnD48GEWLVrEhQsXePvtt21aR0ZGBhkZGSxcuJBp06ZZxg3DICUlBQcHB5u+voiIPH4KRyIiUm8UFBTw2muvkZmZycqVK/H39y/zeFhYGCkpKZw8edLmtVy/fh0AV1fXMuN2dnY0adLE5q8vIiKPn+45EhGReiM+Pp5Lly7xyiuvlAtGpby8vAgODi4ztmfPHoKCgvD29sbHx4egoCD27NlTbq6fnx8hISGkpqYSFhaGj48Pffv25c033yQ7O9tyXEhIiGW1aNGiRZjNZsxmM5mZmZXec1RYWEhkZCS+vr54eXkxceJEDhw4UOnPmpaWxoIFC/D19aVXr174+fkRGRlJfn5+meNK71m6c+cO7733HoMHD8bT05OgoCBOnDhR7nkNwyAuLo5Jkybh4+ODj48Pzz//PJ999lmZ44qKili9ejW/+93v8PT0pF+/foSHh3P69OlKaxYRqe+0ciQiIvXGrl27AJg8eXKV52zYsIElS5bQtWtX3njjDQASExOZPXs2S5YsKfdcWVlZTJ8+nVGjRvH2229z9uxZNm/eTG5uLl9//TUA4eHh9OnTh9WrVzN58mT69u0LgLu7Ozdv3qywjvnz57Nnzx5GjBjBsGHDuHz5MnPmzKF9+/bljj116hQzZszAxcWFyZMn06ZNG86ePUtMTAzHjx8nJiYGR0fHMnNCQ0Nxd3dn9uzZ5OTksG7dOsLCwti7dy/Ozs6W4xYsWEBSUhK9e/cmPDyc5s2bc/HiRXbt2kVERAQA9+7dIzQ0lOPHjzNu3DiCg4PJzc0lLi6OKVOmEBsbi6enZ5X/H4iI1BcKRyIiUm+cP38eZ2dnOnToUKXjb9++zfLly+nYsSPx8fGWkDB16lTGjx/P0qVLGTNmDC4uLpY56enprFixgueee84yZm9vz8aNG7l48SJdu3Zl6NChNGrUiNWrV+Pt7c24ceMsx1YUjg4cOMCePXuYMGECS5cutYz379+f2bNnlzv+D3/4A61btyYhIaFMsBk8eDC///3vSUpK4oUXXigz55lnnuH999+3fN+tWzfmzp3Ljh07CAoKAmDnzp0kJSURGBhIZGQk9vb/fwFJSUmJ5b83bNjA0aNH+eqrrxg2bJhlfOrUqYwdO5Zly5YRExNTwb+4iEj9psvqRESk3sjNzaVZs2ZVPv7gwYPk5+cTEhJSJmQ4OzsTEhJCfn4+hw4dKjPHw8OjTDACGDRoEPAgOFmj9BK+0NDQMuOjRo2iS5cuZcbOnTvHuXPnGDt2LEVFRdy8edPy1bdvX5ycnDh48GC513j55Zd/teakpCQA3nnnnTLBCCjz/XfffUfXrl3p2bNnmdcvKipiyJAhHDt2jIKCgmr+K4iIPPm0ciQiIvWGs7MzeXl5VT4+MzMTgO7du5d7rHQsIyOjzHhFq1Jubm4A5OTkVPm1H5aRkYG9vT2dO3cu91i3bt24dOmS5fvU1FQAVq5cWenfSirdDOJhv6y7RYsW5WpOT0+ndevWtGrV6pH1pqamUlBQwODBgys95tatW7Rt2/aRzyMiUt8oHImISL3RvXt3fvzxRzIyMqp8aV11PWoLbsMwbPKaFZk5c2aZS9oe9vBlgKUqq9uamg3DwGQysWjRokqPcXd3r/bziog86RSORESk3vD39+fHH38kPj6e+fPn/+rxpQHq/Pnz5VZBLly4UOYYW+rQoQMlJSWkpaWVW8UqXSkq1alTJ+DBZW5Dhgx5rHV07tyZvXv3cv369UeuHnXq1Ilbt24xaNCgcpffiYg0ZHrHExGRemPSpEl06dKFr7/+usKtuOHBTm8bNmwAYOjQoTg5OREbG0tubq7lmNzcXGJjY3FycmLo0KE2r3vkyJEArF27tsz4nj17ylxSBw82VjCZTGzatKncJX8AxcXFVl/e9/zzzwPw8ccfl9mAAcquMI0fP57s7GzWrVtX4fNUdFmfiEhDoJUjERGpN5o2bcqaNWsICwtj9uzZ+Pr6MmTIENzc3Lh58yY//PADBw4cYNasWcCDy8/eeustlixZwksvvcSECROAB1t5p6ens2TJEpo3b27zuocNG8aIESNITEwkJyeHYcOGkZGRwebNmzGZTPz73/+2HGtnZ8eyZcuYMWMGgYGBvPjii/z2t7+loKCA9PR0du/ezfz588vtVlcVY8aMITk5mW3btpGeno6fnx8uLi6kpaVx4MABduzYAcD06dM5dOgQy5Yt48iRIwwaNAhnZ2euXLnCkSNHaNy4sXarE5EGSeFIRETqlU6dOrFt2zY2b97Mrl27WL16Nfn5+bi6utKrVy+WLl1qWSEBCA4OxsPDg7Vr1xIdHQ3A008/TXR0NKNGjaq1uqOiooiKiiIpKYlDhw5hMplYuXIlO3bsKBOOAHr06EFiYiJr1qxh3759bNq0iWbNmtGuXTsmTJjwyI0Sfs0nn3xCv379SEhIIDo6Gnt7e9q3b09AQIDlGEdHR9asWcPGjRvZvn27ZWMIDw8PPD09LSFTRKShsTNq8+5SERERERGRJ5TuORIREREREUHhSEREREREBFA4EhERERERARSOREREREREAIUjERERERERQOFIREREREQEUDgSEREREREBFI5EREREREQAhSMRERERERFA4UhERERERASA/wPp9tawmUPu6AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofcRyMHTxh_f",
        "colab_type": "text"
      },
      "source": [
        "### Visualize Attentions\n",
        "\n",
        "For each token, visualize the average over all 12 heads of the last layer's attention to the special character [CLS]. The darker the background of the token, the higher its attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3r6jQpFKWWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lla = last_layer_attentions[0][0][:,0,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDiU0fPDTgkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def avg_token_attentions(last_layer_attentions):\n",
        "  return last_layer_attentions.mean(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iYtEu5vhJPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_token_atts = avg_token_attentions(lla)\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mpp6Q3EHoovo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def handle_special_token_attentions(tokens, avg_token_atts):\n",
        "  new_tokens = []\n",
        "  new_avg_token_atts = []\n",
        "  for i in range(len(tokens)):\n",
        "    if tokens[i].startswith(\"[\") or tokens[i].startswith(\"##\"):\n",
        "      continue\n",
        "    if i < tokenizer.max_len - 1 and tokens[i+1].startswith(\"##\"):\n",
        "      merged_tokens = tokens[i] + tokens[i+1][2:]\n",
        "      atts = [avg_token_atts[i], avg_token_atts[i+1]]\n",
        "      i += 1\n",
        "      while i < tokenizer.max_len - 1 and tokens[i+1].startswith(\"##\"):\n",
        "        merged_tokens += tokens[i+1][2:]\n",
        "        atts.append(avg_token_atts[i+1])\n",
        "        i += 1\n",
        "      new_tokens.append(merged_tokens)\n",
        "      new_avg_token_atts.append(sum(atts)/len(atts))\n",
        "    elif i < tokenizer.max_len - 1:\n",
        "      new_tokens.append(tokens[i])\n",
        "      new_avg_token_atts.append(avg_token_atts[i])\n",
        "  new_avg_token_atts = new_avg_token_atts / sum(new_avg_token_atts)\n",
        "  return new_tokens, new_avg_token_atts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC7Y8NcVq3w3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens, avg_token_atts = handle_special_token_attentions(tokens, avg_token_atts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrCl0RbXmu2W",
        "colab_type": "code",
        "outputId": "3d9c4f12-968a-4b4a-c297-06f54432ef57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "for token, att in zip(tokens, avg_token_atts):\n",
        "  print(token, att)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Heldt 0.05809035311654898\n",
            "hätte 0.054177365889258075\n",
            "kein 0.01076780805190076\n",
            "Problem 0.015572291895335932\n",
            "damit 0.014820633821274184\n",
            ", 0.03917352013893807\n",
            "wenn 0.02144248228174083\n",
            "es 0.018719774092214495\n",
            "in 0.014277498973999304\n",
            "zwei 0.017918605005940766\n",
            "Wochen 0.022517797869843797\n",
            "in 0.0205918796296305\n",
            "der 0.022931140564406836\n",
            "Bundesliga 0.05164067364313314\n",
            "wieder 0.018644275265394195\n",
            "losgehen 0.006203549197879444\n",
            "würde 0.0763474067553108\n",
            ". 0.08695065834329346\n",
            "Wir 0.016695284838822086\n",
            "lernen 0.010095199948753828\n",
            "gerade 0.00937437521288928\n",
            "alle 0.013845376672324487\n",
            ", 0.006552929158801588\n",
            "Kompromisse 0.004703413618638869\n",
            "einzugehen 0.0019346259034978236\n",
            ", 0.007187966658552109\n",
            "an 0.0033253244156550934\n",
            "die 0.00434275681058431\n",
            "wir 0.013029907340620745\n",
            "vor 0.005244916255079734\n",
            "Wochen 0.014071752427553177\n",
            "noch 0.002856268165346162\n",
            "nicht 0.0025236211771747174\n",
            "gedacht 0.0027837738023236\n",
            "haben 0.009560972573564657\n",
            ". 0.03451441453125948\n",
            "Natürlich 0.012998163438054215\n",
            "seien 0.02103701633912061\n",
            "zehn 0.008986962248153032\n",
            "bis 0.018592254079486156\n",
            "14 0.007712328162760276\n",
            "Tage 0.015413732969840456\n",
            "richtiges 0.015905103457055054\n",
            "Mannschaftstraining 0.039522602083170655\n",
            "sinnvoll 0.017404866752708106\n",
            ". 0.025189679597694022\n",
            "Aber 0.014975020892754834\n",
            "vielleicht 0.013304585716822066\n",
            "kriegen 0.011120508241437895\n",
            "wir 0.015696060314527596\n",
            "diese 0.028710521658929653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa9tlNnqvgm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scale_color_h_hex(c_h, scale):\n",
        "    return matplotlib.colors.to_hex(\n",
        "        matplotlib.colors.hsv_to_rgb((c_h, scale, 1)))\n",
        "\n",
        "def blue_background_hex(scale):\n",
        "    return scale_color_h_hex(0.625, scale)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js7LWFLMxoeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "att_html = \"<table><tr>\"\n",
        "for token, att in zip(tokens, avg_token_atts):\n",
        "  att_html += \"<td>\"\n",
        "  att_html += \"<span style=\\\"background-color: \" + blue_background_hex(att) + \"\\\">\" + token + \"</span>\"\n",
        "  att_html += \"</td>\"\n",
        "att_html += \"</tr>\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybnrhz1mzfw1",
        "colab_type": "code",
        "outputId": "3f9f3f2c-1566-45e7-96bb-39b41f4992ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 60
        }
      },
      "source": [
        "IPython.display.HTML(att_html) "
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<table><tr><td><span style=\"background-color: #f0f4ff\">Heldt</span></td><td><span style=\"background-color: #f1f5ff\">hätte</span></td><td><span style=\"background-color: #fcfdff\">kein</span></td><td><span style=\"background-color: #fbfcff\">Problem</span></td><td><span style=\"background-color: #fbfcff\">damit</span></td><td><span style=\"background-color: #f5f8ff\">,</span></td><td><span style=\"background-color: #fafbff\">wenn</span></td><td><span style=\"background-color: #fafbff\">es</span></td><td><span style=\"background-color: #fbfcff\">in</span></td><td><span style=\"background-color: #fafcff\">zwei</span></td><td><span style=\"background-color: #f9fbff\">Wochen</span></td><td><span style=\"background-color: #fafbff\">in</span></td><td><span style=\"background-color: #f9fbff\">der</span></td><td><span style=\"background-color: #f2f5ff\">Bundesliga</span></td><td><span style=\"background-color: #fafbff\">wieder</span></td><td><span style=\"background-color: #fdfeff\">losgehen</span></td><td><span style=\"background-color: #ecf0ff\">würde</span></td><td><span style=\"background-color: #e9eeff\">.</span></td><td><span style=\"background-color: #fbfcff\">Wir</span></td><td><span style=\"background-color: #fcfdff\">lernen</span></td><td><span style=\"background-color: #fdfdff\">gerade</span></td><td><span style=\"background-color: #fbfcff\">alle</span></td><td><span style=\"background-color: #fdfeff\">,</span></td><td><span style=\"background-color: #fefeff\">Kompromisse</span></td><td><span style=\"background-color: #ffffff\">einzugehen</span></td><td><span style=\"background-color: #fdfeff\">,</span></td><td><span style=\"background-color: #fefeff\">an</span></td><td><span style=\"background-color: #fefeff\">die</span></td><td><span style=\"background-color: #fcfdff\">wir</span></td><td><span style=\"background-color: #fefeff\">vor</span></td><td><span style=\"background-color: #fbfcff\">Wochen</span></td><td><span style=\"background-color: #fefeff\">noch</span></td><td><span style=\"background-color: #feffff\">nicht</span></td><td><span style=\"background-color: #fefeff\">gedacht</span></td><td><span style=\"background-color: #fdfdff\">haben</span></td><td><span style=\"background-color: #f6f8ff\">.</span></td><td><span style=\"background-color: #fcfdff\">Natürlich</span></td><td><span style=\"background-color: #fafbff\">seien</span></td><td><span style=\"background-color: #fdfdff\">zehn</span></td><td><span style=\"background-color: #fafbff\">bis</span></td><td><span style=\"background-color: #fdfeff\">14</span></td><td><span style=\"background-color: #fbfcff\">Tage</span></td><td><span style=\"background-color: #fbfcff\">richtiges</span></td><td><span style=\"background-color: #f5f7ff\">Mannschaftstraining</span></td><td><span style=\"background-color: #fbfcff\">sinnvoll</span></td><td><span style=\"background-color: #f9faff\">.</span></td><td><span style=\"background-color: #fbfcff\">Aber</span></td><td><span style=\"background-color: #fcfcff\">vielleicht</span></td><td><span style=\"background-color: #fcfdff\">kriegen</span></td><td><span style=\"background-color: #fbfcff\">wir</span></td><td><span style=\"background-color: #f8faff\">diese</span></td></tr>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJLGG1jmzszh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}